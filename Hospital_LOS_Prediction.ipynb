{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PSamita/MIT-Course/blob/main/Hospital_LOS_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmmAY9ANJdiZ"
      },
      "source": [
        "# **Hospital Length of Stay (LOS) Prediction**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG7RImq5Jnzy"
      },
      "source": [
        "## **Context**\n",
        "\n",
        "Hospital management is a vital area that has garnered significant attention during the COVID-19 pandemic. The inefficient distribution of resources, such as beds and ventilators, can potentially lead to numerous complications. However, this issue can be mitigated by accurately predicting the length of stay (LOS) for patients prior to admission. Once this information is determined, the hospital can effectively plan appropriate treatment, allocate resources, and arrange staff accordingly to reduce LOS and enhance the chances of recovery. Additionally, the allocation of rooms and beds can be optimized based on these predictions.\n",
        "\n",
        "Due to its inefficient management system, HealthPlus hospital has experienced substantial financial losses and loss of life. They have encountered difficulties in equitably allocating equipment, beds, and hospital staff. Implementing a system capable of estimating the length of stay (LOS) for patients could significantly address this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDFFGBh5JuM_"
      },
      "source": [
        "## **Objective**\n",
        "\n",
        "The objective of this project is to analyze the data at HealthPlus as a Data Scientist, determine the key factors that significantly impact the length of stay (LOS), and create a machine learning model capable of predicting a patient's LOS using the data available during admission and after conducting relevant tests. Furthermore, the project aims to derive meaningful insights and develop policies from the data to assist the hospital in improving its healthcare infrastructure and increasing revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY6qQyYnn4av"
      },
      "source": [
        "## **Data Dictionary**\n",
        "\n",
        "The data contains various information recorded during the time of admission of the patient. It only contains records of patients who were admitted to the hospital. The detailed data dictionary is given below:\n",
        "\n",
        "\n",
        "* **patientid**: Patient ID\n",
        "* **Age**: Range of age of the patient\n",
        "* **gender**: Gender of the patient\n",
        "* **Type of Admission**: Trauma, emergency or urgent\n",
        "* **Severity of Illness**: Extreme, moderate, or minor\n",
        "* **health_condition**s: Any previous health conditions suffered by the patient\n",
        "* **Visitors with Patient**: The number of patients who accompany the patient\n",
        "* **Insurance**: Does the patient have health insurance or not?\n",
        "* **Admission_Deposit**: The deposit paid by the patient during admission\n",
        "* **Stay (in days)**: The number of days that the patient has stayed in the hospital. This is the **target variable**\n",
        "* **Available Extra Rooms in Hospital**: The number of rooms available during admission\n",
        "* **Department**: The department which will be treating the patient\n",
        "* **Ward_Facility_Code**: The code of the ward facility in which the patient will be admitted\n",
        "* **doctor_name**: The doctor who will be treating the patient\n",
        "* **staff_available**: The number of staff who are not occupied at the moment in the ward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HwCLXEdleNm"
      },
      "source": [
        "## **Approach to solve the problem**\n",
        "\n",
        "1. Import the necessary libraries.\n",
        "2. Read the dataset and get an overview.\n",
        "3. Conduct an exploratory data analysis.\n",
        "4. Conduct data preprocessing.\n",
        "5. Define the performance metric and build ML models\n",
        "6. Check for assumptions\n",
        "7. Compare models and determine the best one.\n",
        "8. Report observations and business insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh6pbSWLJ7pT"
      },
      "source": [
        "## **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VS_eRGE5yfLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu98q-R-KFfP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Removes the limit for the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "# Sets the limit for the number of displayed rows\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "# To build models for prediction\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#for tuning the model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# To check model performance\n",
        "from sklearn.metrics import make_scorer,mean_squared_error, r2_score, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvautAjCU6Im"
      },
      "outputs": [],
      "source": [
        "#read the healthcare dataset file\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/MIT Data Science/healthcare_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAW7LbQGDzc_"
      },
      "outputs": [],
      "source": [
        "# copying data to another variable to avoid any changes to original data\n",
        "same_data = data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUEuNqtbPVT0"
      },
      "source": [
        "## **Data Overview**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WBP3qyFFlAM"
      },
      "outputs": [],
      "source": [
        "# View the first 5 rows of the dataset\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-JcE75QFv6o"
      },
      "outputs": [],
      "source": [
        "# View the last 5 rows of the dataset\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh0JY34cdYHB"
      },
      "outputs": [],
      "source": [
        "#Understand the shape of the data\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKUvaKzkdjUc"
      },
      "source": [
        "- The dataset has **5,00,000 rows and 15 columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shfF1UM7Ke5E"
      },
      "outputs": [],
      "source": [
        "#Checking the info of the data\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LQeBs4sPHNK"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "-  Available Extra Rooms in Hospital , staff_available, patientid, Visitors with Patient, Admission_Deposit, and Stay (in days) are of **numeric data type** and the rest of the columns are of **object data type*.\n",
        "- The number of non-null values is the same as the total number of entries in the data i.e. **there are no null values.**\n",
        "- The column patientid is an identifier for patients in the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z73h7AF9r1PM"
      },
      "outputs": [],
      "source": [
        "# checking for duplicate values in the Data\n",
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp4_mcJjP3LE"
      },
      "source": [
        "**Observations:** \n",
        "- Data has unique rows only. There is no need to remove any rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Xba7tEO9KE"
      },
      "outputs": [],
      "source": [
        "#To view patientid and the number of times they have visited the hospital\n",
        "data['patientid'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pgZWSL-1vS5"
      },
      "source": [
        "**Observations:**\n",
        "- **The maximum number of times the same patient admitted to the hospital is 21 and minimum is 1.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIvXLbuuSuEE"
      },
      "outputs": [],
      "source": [
        "#Checking the descriptive statistics of the columns\n",
        "data.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfliAV--0q5"
      },
      "source": [
        "**Observations :**\n",
        "\n",
        "* There are around **3 rooms available in the hospital on an average** and there are times when the hospital is full and there are no rooms available. The **maximum number of rooms available in the hospital are 24**.\n",
        "* **On average there are around 5 staff personal available to treat the new patients** but it can also be zero at times. The maximum staff available in the hospital is 10.\n",
        "* **On average around 3 visitors accompany the patient.** Some patients come on their own (minimum zero) and few cases have 32 visitors. It will be interesting to see if there is any relation between number of visitors and the severity of the patient.\n",
        "* **The average admission deposit lies around 4722 dollars and a minimum of 1654 dollars is paid on every admission.**\n",
        "* **Patient's stay has a large range from 3 to 51 days.** There might be outliers in this variable. The median length of stay is 9 days.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYe3XYCFS1ru"
      },
      "outputs": [],
      "source": [
        "# List of all important categorical variables\n",
        "cat_col = [\"Department\", \"Type of Admission\", 'Severity of Illness', 'gender', 'Insurance', 'health_conditions', 'doctor_name', \"Ward_Facility_Code\", \"Age\"]\n",
        "\n",
        "# Printing the number of occurrences of each unique value in each categorical column\n",
        "for column in cat_col:\n",
        "    print(data[column].value_counts(1))\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSIXtFmtgeNJ"
      },
      "source": [
        "**Observations :**\n",
        "\n",
        "- **The majority of patients (~82%) admit to the hospital with moderate and minor illness** which is understandable as extreme illness is less frequent than moderate and minor illness. \n",
        "- **Gynecology department gets the most number of patients (~68%)** in the hospital whereas patients in Surgery department are very less (~1%).\n",
        "- **Ward A and C accommodate the least number of patients (~12%).** These might be wards reserved for patient with extreme illness and patients who need surgery. It would be interesting to see if patients from these wards also stay for longer duration.\n",
        "- **The majority of patients belong to the age group of 21-50 (\\~75%) and are women (~75%).** The most number of patients in the gynecology department of the hospital can justify this.\n",
        "- Most of the patients admitted to the hospital are the cases of trauma (~62%).\n",
        "- **High Blood pressure and diabetes are the most common health conditions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTHU4IPogofK"
      },
      "source": [
        "## **Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU_-9VCyskuV"
      },
      "source": [
        "### **Univariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FXeuDBXU3Dv"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeEmAqhGBlQ3"
      },
      "source": [
        "#### **Length of stay**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZavZx-IU47X"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(data, \"Stay (in days)\", kde=True, bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f59iZeHCE9m"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- There are **fewer number of patients staying more than 10 days in the hospital and very few who stay for more than 40 days**. This might be because the majority of patients are admitted for moderate or minor illness. \n",
        "- The peak of the distribution shows that the the most of the patients stay for 8-9 days in the hospital. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwYRba3GCmMR"
      },
      "source": [
        "#### **Admission deposit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45-b0yudsw8V"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(data, \"Admission_Deposit\", kde=True, bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iuWUHFtC40E"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- The **distribution of the admission fees is close to normal with outliers on both sides**. There are few patients paying high amount of admission fees and few patients paying low amount of admission fees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnawqWwiDGAN"
      },
      "source": [
        "#### **Visitors with patients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaFuytims4mK"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(data, \"Visitors with Patient\", kde=True, bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDwBBE2eDOxo"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- The distribution of the number of visitors with the patient is **highly skewed towards right**.\n",
        "- **2 and 4 are the most common number of visitors with patients.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5PnzTiBuJA5"
      },
      "source": [
        "### **Bivariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARmMlB5IO3UQ"
      },
      "outputs": [],
      "source": [
        "#finding the correlation between various columns of the dataset\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\",cmap=\"Spectral\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXjMP0xgPHoW"
      },
      "source": [
        "**Observations:** \n",
        "- The heatmap shows that there is **no correlation between variables**.\n",
        "- The continuous variables shows no correlation with the target variable (Stay (in days)) which indicates that the **categorical variables might be more important for the prediction.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbM2637QiV0D"
      },
      "outputs": [],
      "source": [
        "# function to plot stacked bar plots\n",
        "\n",
        "\n",
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\",\n",
        "        frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7g781gtwOV2"
      },
      "source": [
        "**Let's start by checking the distribution of the LOS for the various wards**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3C3qUsTlIpK"
      },
      "outputs": [],
      "source": [
        "sns.barplot(y = 'Ward_Facility_Code',x = 'Stay (in days)',data = data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ScD8h0FHRxX"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- The hypothesis we made earlier is correct i.e. **wards A and C has the patients staying for the longest duration.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH3lFpuU4I6B"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data, \"Ward_Facility_Code\", \"Department\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26J9ZLFC4WO7"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- **Ward Facility B, D and F are dedicated only to the gynecology department.**\n",
        "- Wards A, C, and E have patients with all other diseases and patients undergoing surgery are admitted in ward A only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So2J-eKbxLXE"
      },
      "source": [
        "**Usually, the more severe the illness, the more the LOS, let's check the distribution of severe patients in various wards**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_Kr-34iXUp"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data, \"Ward_Facility_Code\", \"Severity of Illness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiDH49bWIAJF"
      },
      "source": [
        "**Observations :**\n",
        "\n",
        "- Ward A has the most extreme case as well as the longest length of stay in the hospital. It might require more staff and resources as compared to other wards.\n",
        "- Ward F has a large number of minor cases and Ward E has large number of moderate cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4pmLBDiyJ4c"
      },
      "source": [
        "**Age can also be an important factor to find the length of stay. Let's check the same.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmSuZ_18n7Ch"
      },
      "outputs": [],
      "source": [
        "sns.barplot(y = 'Age',x = 'Stay (in days)',data = data )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsYpNHK2It36"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- **Patients aged between 1-10 and 51-100 tend to stay the most number of days in the hospital.** This might be because the majority of the patients between 21-50 age group gets admitted to the gynecology department and patients in age groups 1-10 and 5-100 might get admitted due to some other serious illness. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTjAF7lA2A9v"
      },
      "source": [
        "## **Data Preparation for Model Building**\n",
        "\n",
        "- Before we proceed to build a model, we'll have to encode categorical features.\n",
        "- Separate the independent variables and dependent Variable.\n",
        "- We'll split the data into train and test to be able to evaluate the model that we train on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9sae8pN9cGT"
      },
      "outputs": [],
      "source": [
        "# Creating dummy variables for the categorical columns\n",
        "#drop_first=True is used to avoid redundant variables\n",
        "data = pd.get_dummies(\n",
        "    data,\n",
        "    columns=data.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),\n",
        "    drop_first=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri-VtxAj9fdT"
      },
      "outputs": [],
      "source": [
        "#Check the data, after handling categorical data\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM40YACV7O5j"
      },
      "outputs": [],
      "source": [
        "#Dropping patientid from the data as it is an identifier and will not add value to the analysis\n",
        "data=data.drop(columns=[\"patientid\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpnezOEgjQY7"
      },
      "outputs": [],
      "source": [
        "#Separating independent variables and the target variable\n",
        "x=data.drop('Stay (in days)',axis=1)\n",
        "\n",
        "y=data['Stay (in days)'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTg4-p8-9EO7"
      },
      "outputs": [],
      "source": [
        "#Splitting the dataset into train and test datasets\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,shuffle=True,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEG6Rs2wjVrO"
      },
      "outputs": [],
      "source": [
        "#Checking the shape of the train and test data\n",
        "print(\"Shape of Training set : \", x_train.shape)\n",
        "print(\"Shape of test set : \", x_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwDclxhA8ply"
      },
      "source": [
        "## **Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH1_ef6jSVcD"
      },
      "source": [
        "* We will be using different metrics functions defined in sklearn like RMSE, MAE, and ð‘…2 for regression models evaluation.\n",
        "* We will define a function to calculate MAPE and adjusted ð‘…2.\n",
        "* The mean absolute percentage error (MAPE) measures the accuracy of predictions as a percentage, and can be calculated as the average absolute percent error for each predicted value minus actual values divided by actual values. It works best if there are no extreme values in the data and none of the actual values are 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4s83hAM9I8X"
      },
      "outputs": [],
      "source": [
        "# function to compute adjusted R-squared\n",
        "def adj_r2_score(predictors, targets, predictions):\n",
        "    r2 = r2_score(targets, predictions)\n",
        "    n = predictors.shape[0]\n",
        "    k = predictors.shape[1]\n",
        "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "\n",
        "# function to compute MAPE\n",
        "def mape_score(targets, predictions):\n",
        "    return np.mean(np.abs(targets - predictions) / targets) * 100\n",
        "\n",
        "\n",
        "# function to compute different metrics to check performance of a regression model\n",
        "def model_performance_regression(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check regression model performance\n",
        "\n",
        "    model: regressor\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    r2 = r2_score(target, pred)  # to compute R-squared\n",
        "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
        "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
        "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
        "    mape = mape_score(target, pred)  # to compute MAPE\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae,\n",
        "            \"R-squared\": r2,\n",
        "            \"Adj. R-squared\": adjr2,\n",
        "            \"MAPE\": mape,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gmJkAMv2pMK"
      },
      "outputs": [],
      "source": [
        "# RMSE\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((targets - predictions) ** 2).mean())\n",
        "\n",
        "\n",
        "# MAPE\n",
        "def mape(predictions, targets):\n",
        "    return np.mean(np.abs((targets - predictions)) / targets) * 100\n",
        "\n",
        "\n",
        "# MAE\n",
        "def mae(predictions, targets):\n",
        "    return np.mean(np.abs((targets - predictions)))\n",
        "\n",
        "\n",
        "# Model Performance on test and train data\n",
        "def model_pref(olsmodel, x_train, x_test, y_train,y_test):\n",
        "\n",
        "    # Insample Prediction\n",
        "    y_pred_train = olsmodel.predict(x_train)\n",
        "    y_observed_train = y_train\n",
        "\n",
        "    # Prediction on test data\n",
        "    y_pred_test = olsmodel.predict(x_test)\n",
        "    y_observed_test = y_test\n",
        "\n",
        "    print(\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"Data\": [\"Train\", \"Test\"],\n",
        "                \"RMSE\": [\n",
        "                    rmse(y_pred_train, y_observed_train),\n",
        "                    rmse(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "                \"MAE\": [\n",
        "                    mae(y_pred_train, y_observed_train),\n",
        "                    mae(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "                \"MAPE\": [\n",
        "                    mape(y_pred_train, y_observed_train),\n",
        "                    mape(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "            }\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeLDFrO32pML",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Statsmodel api does not add a constant by default. We need to add it explicitly.\n",
        "x_train1 = sm.add_constant(x_train)\n",
        "# Add constant to test data\n",
        "x_test1 = sm.add_constant(x_test)\n",
        "\n",
        "# create the model\n",
        "olsmodel1 = sm.OLS(y_train, x_train1).fit()\n",
        "\n",
        "# get the model summary\n",
        "olsmodel1.summary()\n",
        "print(olsmodel1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z-AWRfw2pMN"
      },
      "outputs": [],
      "source": [
        "lin_reg_test = model_performance_regression(olsmodel1, x_test1, y_test)\n",
        "lin_reg_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEUsT8xc2pMN"
      },
      "source": [
        "- We can see that `R-squared` for the model is `0.84`. \n",
        "- Not all the variables are statistically significant enough to predict the outcome variable. To check which ones are statistically significant or have enough predictive power to predict the target variable, we need to check the `p-value` against all the independent variables.\n",
        "\n",
        "**Interpreting the Regression Results:**\n",
        "\n",
        "1. **Adjusted R-squared**: It reflects the fit of the model.\n",
        "    - R-squared values range from 0 to 1, where a higher value generally indicates a better fit, assuming certain conditions are met.\n",
        "    - In our case, the value for Adj. R-squared is **0.84**\n",
        "\n",
        "2. **coef**: It represents the change in the output Y due to a change of one unit in the variable (everything else held constant).\n",
        "3. **std err**: It reflects the level of accuracy of the coefficients.\n",
        "    - The lower it is, the more accurate the coefficients are.\n",
        "4. **P > |t|**: The p-value:\n",
        "   \n",
        "   * Pr(>|t|) : For each independent feature there is a null hypothesis and alternate hypothesis \n",
        "\n",
        "    **Ho:** Null Hypothesis - The independent feature is not significant \n",
        "   \n",
        "    **Ha:** Alternate Hypothesis - The independent feature is significant \n",
        "    \n",
        "   * A p-value of less than 0.05 is considered to be statistically significant.\n",
        "\n",
        "   \n",
        "5. **Confidence Interval**: It represents the range in which our coefficients are likely to fall (with a likelihood of 95%).\n",
        "\n",
        "\n",
        "\n",
        "* Both the **R-squared and Adjusted R-squared of the model are around 84%**. This is a clear indication that we have been able to create a good model that is able to explain variance in the LOS of patients for up to 84%.\n",
        "\n",
        "* We can examine the significance of the regression model, and try dropping insignificant variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMRY5SoA2pMO"
      },
      "outputs": [],
      "source": [
        "model_pref(olsmodel1, x_train1, x_test1,y_train,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYh_DeQl2pMO"
      },
      "source": [
        "**The Root Mean Squared Error** of train and test data is **not different**, indicating that **our model is not overfitting** to the training data.\n",
        "\n",
        "Mean Absolute Error (MAE) indicates that our current model is able to predict LOS of patients within **mean error of 2.15 days** on the test data.\n",
        "\n",
        "The units of both RMSE and MAE are same - days in this case. But RMSE is greater than MAE because it penalizes the outliers more.\n",
        "\n",
        "**Mean Absolute Percentage Error is ~19%** on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_F6m_ytvFQt"
      },
      "source": [
        "### Checking for Multicollinearity \n",
        "\n",
        "We will use VIF, to check if there is multicollinearity in the data.\n",
        "\n",
        "Features having a VIF score >5 will be dropped/treated till all the features have a VIF score <5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om72IpjAvFQt"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "\n",
        "def checking_vif(train):\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"feature\"] = train.columns\n",
        "\n",
        "    # calculating VIF for each feature\n",
        "    vif[\"VIF\"] = [\n",
        "        variance_inflation_factor(train.values, i) for i in range(len(train.columns))\n",
        "    ]\n",
        "    return vif\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvruryCPvFQt"
      },
      "outputs": [],
      "source": [
        "print(checking_vif(x_train1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnS7l50wvFQt"
      },
      "outputs": [],
      "source": [
        "# create the model\n",
        "olsmodel1 = sm.OLS(y_train, x_train1).fit()\n",
        "\n",
        "# get the model summary\n",
        "olsmodel1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C93HsknvFQu"
      },
      "source": [
        "It is not a good practice to consider VIF values for dummy variables as they are correlated to other categories and hence have a high VIF usually. So we built the model and calculated the p-values. We can see that the categories in Insurance_Yes column shows p-value higher than 0.05. So we can drop that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbkMefpvvFQu"
      },
      "outputs": [],
      "source": [
        "x_train2 = x_train1.drop(['Insurance_Yes','staff_available','Visitors with Patient'] , axis=1)\n",
        "print(checking_vif(x_train2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtfJkhgr2pMP"
      },
      "source": [
        "## **Dropping the insignificant variables and creating the regression model again**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvDa_4-i2pMP"
      },
      "source": [
        "### **Examining the significance of the model**\n",
        "\n",
        "It is not enough to just fit a multiple regression model to the data, it is also necessary to check whether all the regression coefficients are significant or not. Significance here means whether the population regression parameters are significantly different from zero.\n",
        "\n",
        "From the above it may be noted that the regression coefficients corresponding to staff_available, Visitors with Patient, and Insurance_Yes **are not statistically significant at level Î± = 0.05.** In other words, the regression coefficients corresponding to these three are not significantly different from 0 in the population. \n",
        "\n",
        "Suppose you have a nominal categorical variable having 4 categories (or levels). You would create 3 dummy variables (k-1 = 4-1 dummy variables) and set one category as a reference level. Suppose one of them is insignificant.  Then if you exclude that dummy variable, it would change the reference level as you are indirectly combining that insignificant level with the original reference level. It would have a new reference level and interpretation would change. Moreover, excluding the level may make the others insignificant. If all the categories in a column shows p-value higher than 0.05 we can drop that.\n",
        "\n",
        "**Hence, we will eliminate these three features and create a new model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqm3MODN2pMP"
      },
      "outputs": [],
      "source": [
        "# create the model after dropping variables\n",
        "x_train2 = x_train1.drop(['Insurance_Yes','staff_available','Visitors with Patient'] , axis=1)\n",
        "x_test2 = x_test1.drop(['Insurance_Yes','staff_available','Visitors with Patient'] , axis=1)\n",
        "# create the model\n",
        "olsmodel2 = sm.OLS(y_train, x_train2).fit()\n",
        "# get the model summary\n",
        "olsmodel2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U00tOIA2pMQ"
      },
      "outputs": [],
      "source": [
        "lin_reg_test = model_performance_regression(olsmodel2, x_test2, y_test)\n",
        "lin_reg_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16fOWNF62pMS"
      },
      "source": [
        "### **Checking the performance of the model on the train and test data set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjpqPUcw2pMS"
      },
      "outputs": [],
      "source": [
        "model_pref(olsmodel2, x_train2, x_test2,y_train,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O99RtHrB2pMT"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "* RMSE, MAE, and MAPE of train and test data are not very different, indicating that the **model is not overfitting and has generalized well.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qcOig1QvFQv"
      },
      "source": [
        "## **Checking for assumptions and rebuilding the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTcGvlR7vFQv"
      },
      "source": [
        "In this step, we will check for the below assumptions in the model, to check if they hold true or not. And if there is any issue, then we will rebuild the model after fixing those issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuWiy8kyvFQw"
      },
      "source": [
        "\n",
        "1. Mean of residuals should be 0\n",
        "2. No Heteroscedasticity\n",
        "3. Linearity of variables\n",
        "4. Normality of error terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pffiMrAgvFQw"
      },
      "source": [
        "### **Mean of residuals should be 0 and normality of error terms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wozTeoq0vFQw"
      },
      "outputs": [],
      "source": [
        "residual = olsmodel2.resid # Residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmMfx7J5vFQw"
      },
      "outputs": [],
      "source": [
        "residual.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcPysVyIvFQw"
      },
      "source": [
        "The mean of residuals is very close to 0. Hence, the corresponding assumption is satisfied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFpfWG-FvFQx"
      },
      "source": [
        "## **Tests for Normality**\n",
        "\n",
        "**What is the test?**\n",
        "\n",
        "* Error terms/Residuals should be normally distributed\n",
        "\n",
        "* If the error terms are non- normally distributed, confidence intervals may become too wide or narrow. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficients based on minimization of least squares.\n",
        "\n",
        "**What do non-normality indicate?**\n",
        "\n",
        "* It suggests that there are a few unusual data points which must be studied closely to make a better model.\n",
        "\n",
        "**How to check the normality?**\n",
        "\n",
        "* It can be checked via QQ Plot. Residuals following normal distribution will make a straight line plot otherwise not.\n",
        "\n",
        "* Other test to check for normality : Shapiro-Wilk test.\n",
        "\n",
        "**What is the residuals are not-normal?**\n",
        "\n",
        "* We can apply transformations like log, exponential, arcsinh, etc as per our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeGltEDhvFQx"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of residuals\n",
        "sns.histplot(residual, kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqj0byS3vFQx"
      },
      "source": [
        "The residuals have a close to normal distribution. The assumption of normality is satisfied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-L33yqOvFQx"
      },
      "source": [
        "### **Linearity of Variables**\n",
        "\n",
        "It states that the predictor variables must have a linear relation with the dependent variable.\n",
        "\n",
        "To test the assumption, we'll plot residuals and fitted values on a plot and ensure that residuals do not form a strong pattern. They should be randomly and uniformly scattered on the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpDSPPsBvFQx"
      },
      "outputs": [],
      "source": [
        "# predicted values\n",
        "fitted = olsmodel2.fittedvalues\n",
        "\n",
        "# sns.set_style(\"whitegrid\")\n",
        "sns.residplot(x = fitted, y = residual, color=\"lightblue\")\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual PLOT\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZfVi9v5vFQy"
      },
      "source": [
        "**Observations**\n",
        "- We can see that there is no pattern in the residuals vs fitted values scatter plot now i.e. the linearity assumption is satisfied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNECp3KIvFQy"
      },
      "source": [
        "Let's check the final assumption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "341TVpJRvFQy"
      },
      "source": [
        "### **No Heteroscedasticity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcb8xdFEvFQy"
      },
      "source": [
        "#### **Test for Homoscedasticity**\n",
        "\n",
        "* **Homoscedasticity -** If the variance of the residuals are symmetrically distributed across the regression line, then the data is said to homoscedastic.\n",
        "\n",
        "* **Heteroscedasticity -** If the variance is unequal for the residuals across the regression line, then the data is said to be heteroscedastic. In this case the residuals can form an arrow shape or any other non symmetrical shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eSSf03svFQz"
      },
      "source": [
        "We will use Goldfeldâ€“Quandt test to check homoscedasticity.\n",
        "\n",
        "Null hypothesis : Residuals are homoscedastic\n",
        "\n",
        "Alternate hypothesis : Residuals are hetroscedastic\n",
        "\n",
        "alpha = 0.05 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w75UUJqgvFQz"
      },
      "outputs": [],
      "source": [
        "import statsmodels.stats.api as sms\n",
        "from statsmodels.compat import lzip\n",
        "\n",
        "name = [\"F statistic\", \"p-value\"]\n",
        "test = sms.het_goldfeldquandt(residual, x_train2)\n",
        "lzip(name, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DiIElq_vFQz"
      },
      "source": [
        "As we can see from the above test the p-value is greater than 0.05, so we fail to reject the null-hypothesis. That means - residuals are homoscedastic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ChPecnvFQz"
      },
      "outputs": [],
      "source": [
        "coef = olsmodel2.params\n",
        "coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5plW7GoSvFQ0"
      },
      "outputs": [],
      "source": [
        "# Let us write the equation of the fit\n",
        "Equation = \"Stay (in days)=\"\n",
        "print(Equation, end='\\t')\n",
        "for i in range(len(coef)):\n",
        "    print('(', coef[i], ') * ', coef.index[i], '+', end = ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDggSWcb2pMV"
      },
      "source": [
        "### **Interpreting our Regression Coefficients**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoafnTOA2pMW"
      },
      "source": [
        "With our linear regression model's adjusted R-squared value of around 0.84, we are able to capture **84% of the variation** in our data.\n",
        "\n",
        "The p-values for these variables are < 0.05 in our final model, meaning they are statistically significant towards Stay (in days) prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qty2K1BP2pMW"
      },
      "source": [
        "* The Stay (in days) decreases with an increase in Department_radiotherapy. 1 unit increase in the Department_radiotherapy leads to a decrease of Stay (in days) ~ 4.62 times the Stay (in days) than the Department_TB&Chest_Disease that serves as a reference variable when everything else is constant.\n",
        "\n",
        "* The Stay (in days) increases with an increase in Department_anesthesia. 1 unit increase in Department_anesthesia leads to an increase of Stay (in days) ~ 6.08 times the Stay (in days) than the Department_TB&Chest_Disease that serves as a reference variable when everything else is constant. This is understandable, as anesthesia is used in severe cases which results in more days of stay.\n",
        "\n",
        "* The Stay (in days) increases with an increase in Department_surgery. 1 unit increase in Department_surgery leads to an increase of Stay (in days) ~ 9.68 times the Stay (in days) than the Department_TB&Chest_Disease that serves as a reference variable when everything else is constant. This is understandable, as surgery is conducted in severe cases which results in more days of stay.\n",
        "\n",
        "* The Stay (in days) increases with an increase in doctor_name_Dr Simon. 1 unit increase in doctor_name_Dr Simon leads to an increase of Stay (in days) ~ 6.14 times the Stay (in days) than the doctor_name_Dr Isaac that serves as a reference variable when everything else is constant. This is understandable, as surgery cases are handled by Dr.Simon.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVWP-AHZ2pMT"
      },
      "source": [
        "### **Applying the cross validation technique to improve the model and evaluating it using different evaluation metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YBZBytY2pMU"
      },
      "outputs": [],
      "source": [
        "# import the required function\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# build the regression model using Sklearn Linear regression\n",
        "linearregression = LinearRegression()                                    \n",
        "\n",
        "cv_Score11 = cross_val_score(linearregression, x_train2, y_train, cv = 10) #cv=10 represents data is divided into 10 folds.\n",
        "cv_Score12 = cross_val_score(linearregression, x_train2, y_train, cv = 10, \n",
        "                             scoring = 'neg_mean_squared_error')                                  \n",
        "\n",
        "\n",
        "print(\"RSquared: %0.3f (+/- %0.3f)\" % (cv_Score11.mean(), cv_Score11.std() * 2))\n",
        "print(\"Mean Squared Error: %0.3f (+/- %0.3f)\" % (-1*cv_Score12.mean(), cv_Score12.std() * 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L72sWJZ-2pMU"
      },
      "source": [
        "**Observations:**\n",
        "- The R-squared on the cross validation is 0.843, whereas on the training dataset it was 0.843\n",
        "- And the MSE on cross validation is 9.831, whereas on the training dataset also it was 9.83"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeoIi1Vm2pMV"
      },
      "source": [
        "We may want to reiterate the model building process again with new features or better feature engineering to increase the R-squared and decrease the MSE on cross validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOXNtE552pMW"
      },
      "source": [
        "### **Ridge Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qx3FqS42pMW"
      },
      "outputs": [],
      "source": [
        "rdg = Ridge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nddtcYWm2pMW"
      },
      "outputs": [],
      "source": [
        "rdg.fit(x_train, y_train)\n",
        "model_pref(rdg, x_train, x_test,y_train,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxyXqnYt2pMX"
      },
      "outputs": [],
      "source": [
        "ridge_regression_perf_test = model_performance_regression(rdg, x_test, y_test)\n",
        "\n",
        "ridge_regression_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugwks0m92pMY"
      },
      "source": [
        "Ridge regression is able to produce similar results in comparison to Linear Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcSnMrJM2pMY"
      },
      "source": [
        "There is definitely some scope to improve the model's performance, as there is a feeling that we are not able to fully model the relationship using linear models.\n",
        "\n",
        "Let's now build **Non-Linear Regression models** like Decision Tree Regressors and Random Forest Regressors and check their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Nkflof9xLs"
      },
      "source": [
        "### **Decision Tree Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qshkBU0uUewv"
      },
      "outputs": [],
      "source": [
        "#Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state = 1)\n",
        "\n",
        "#Fitting the model\n",
        "dt_regressor.fit(x_train,y_train)\n",
        "\n",
        "# Model Performance on test data i.e prediction\n",
        "dt_regressor_perf_test = model_performance_regression(dt_regressor, x_test, y_test)\n",
        "\n",
        "dt_regressor_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa8z6Ric93Qc"
      },
      "source": [
        "### **Tuning the Decision Tree Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ1ncLJuuE3c"
      },
      "outputs": [],
      "source": [
        "# Choose the type of regressor \n",
        "dtree_tuned = DecisionTreeRegressor(random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {'max_depth': np.arange(2,8), \n",
        "              'criterion': ['squared_error', 'friedman_mse'],\n",
        "              'min_samples_leaf': [1, 3, 5, 7],\n",
        "              'max_leaf_nodes' : [2, 5, 7] + [None]\n",
        "             }\n",
        "\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = make_scorer(r2_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer,cv=5)\n",
        "grid_obj = grid_obj.fit(x_train,y_train)\n",
        "\n",
        "# Set the dtree_tuned_regressor to the best combination of parameters\n",
        "dtree_tuned_regressor = grid_obj.best_estimator_\n",
        "\n",
        "dtree_tuned_regressor.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OuST6SBpTSf"
      },
      "source": [
        "**We have tuned the model and fit the tuned model on the training data. Now, let's check the model performance on the testing data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JCMcfuBB68v"
      },
      "outputs": [],
      "source": [
        "#Get the score of tuned decision tree regressor\n",
        "dtree_tuned_regressor_perf_test = model_performance_regression(dtree_tuned_regressor, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qj6zqcuCArm"
      },
      "outputs": [],
      "source": [
        "dtree_tuned_regressor_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-2fDKtEmXhz"
      },
      "source": [
        "**Let's look at the feature importance of the tuned decision tree model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7vyv1jkmoRS"
      },
      "outputs": [],
      "source": [
        "#Plotting the feature importance\n",
        "features = list(x.columns)\n",
        "importances = dtree_tuned_regressor.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sogCEzntm2fF"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- **Department_gynecology, Age_41_50, and Age_31_40 are the most important features** followed by Department_anesthesia, Department_anesthesia, Department_surgery.\n",
        "- The rest of the variables have no impact in this model to decide the duration of the stay in Hospital."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkXRAKBseIfT"
      },
      "source": [
        "### **Bagging Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrPnEn_pi0Ap"
      },
      "outputs": [],
      "source": [
        "#Bagging Regressor\n",
        "bagging_estimator=BaggingRegressor(random_state=1)\n",
        "\n",
        "#Fitting the model\n",
        "bagging_estimator.fit(x_train,y_train)\n",
        "\n",
        "# Model Performance on test data i.e prediction\n",
        "bagging_estimator_perf_test = model_performance_regression(bagging_estimator, x_test, y_test)\n",
        "\n",
        "bagging_estimator_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h_IMznmkhTg"
      },
      "source": [
        "### **Tuned Bagging Regressor** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mwlkvFEklU1"
      },
      "outputs": [],
      "source": [
        "# Choose the type of regressor. \n",
        "bagging_tuned = BaggingRegressor(random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\"n_estimators\": [10,15,20],\n",
        "              \"max_samples\" : [0.8,1],\n",
        "              \"max_features\" : [0.8,1]\n",
        "             }\n",
        "\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = make_scorer(r2_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(bagging_tuned , parameters, scoring=scorer,cv=5)\n",
        "grid_obj = grid_obj.fit(x_train,y_train)\n",
        "\n",
        "# Set the bagging_tuned_regressor to the best combination of parameters\n",
        "bagging_tuned_regressor = grid_obj.best_estimator_\n",
        "\n",
        "bagging_tuned_regressor.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCF5pabCkNyF"
      },
      "outputs": [],
      "source": [
        "#Get the score of tuned Bagging tree regressor\n",
        "bagging_tuned_regressor_perf_test = model_performance_regression(bagging_tuned_regressor, x_test, y_test)\n",
        "bagging_tuned_regressor_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB3zeBib2pMb"
      },
      "source": [
        "**The bagging regressor has no attribute to calculate feature importance in Sklearn. So, let's move on to the next model** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZg2DYmNeQ-2"
      },
      "source": [
        "### **Random Forest Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AWy2uKodOdL"
      },
      "outputs": [],
      "source": [
        "#Random Forest Regressor\n",
        "regressor = RandomForestRegressor(n_estimators = 100, random_state = 1)\n",
        "\n",
        "#Fitting the model\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "# Model Performance on test data i.e prediction\n",
        "regressor_perf_test = model_performance_regression(regressor, x_test, y_test)\n",
        "\n",
        "regressor_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u0gWZX1obu7"
      },
      "source": [
        "### **Tuned Random Forest Regressor**\n",
        "\n",
        "**NOTE:** Due to the large number of observations in the data, the code in the next cell for GridSearchCV might take 1-2 hours to run depending on the configuration of your system.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONfg_OIGoaLU"
      },
      "outputs": [],
      "source": [
        "rf_tuned = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\"n_estimators\": [110, 120],\n",
        "    \"max_depth\": [5, 7],\n",
        "    \"max_features\": [0.8, 1]\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = make_scorer(r2_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5)\n",
        "grid_obj = grid_obj.fit(x_train,y_train)\n",
        "\n",
        "# Set the rf_tuned_regressor to the best combination of parameters\n",
        "rf_tuned_regressor = grid_obj.best_estimator_\n",
        "\n",
        "rf_tuned_regressor.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSh5GQzatVoR"
      },
      "outputs": [],
      "source": [
        "# Model Performance on test data i.e prediction\n",
        "rf_tuned_regressor_perf_test = model_performance_regression(rf_tuned_regressor, x_test, y_test)\n",
        "\n",
        "rf_tuned_regressor_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxhMeC6S2pMd"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- The performance of the tuned model is actually decreased as compared to the model with default parameters. \n",
        "- This might be because we have tried very less number of hyperparameters and less number of values due to computational limits. We can certainly try to improve the performance of the model by tuning the model further.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BwR44Moo7zT"
      },
      "source": [
        "**Visualizing the feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpKYCHSAo619"
      },
      "outputs": [],
      "source": [
        "#Plotting the feature importance\n",
        "importances = rf_tuned_regressor.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhc3EpOs2pMe"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- The top 3 important variables are the same as decision tree model. There are some other features like gender, and doctors adding some value to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_TAsHcN7f_O"
      },
      "source": [
        "**Model Performance Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU-67j8J7fUT"
      },
      "outputs": [],
      "source": [
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        lin_reg_test.T,\n",
        "        ridge_regression_perf_test.T,\n",
        "        dt_regressor_perf_test.T,\n",
        "        regressor_perf_test.T,\n",
        "        bagging_estimator_perf_test.T,\n",
        "        dtree_tuned_regressor_perf_test.T,\n",
        "        bagging_tuned_regressor_perf_test.T,\n",
        "        rf_tuned_regressor_perf_test.T\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "models_test_comp_df.columns = [\n",
        "    \"Linear Regression\",\n",
        "    \"Ridge Regression\",\n",
        "    \"Decision tree regressor\",\n",
        "    \"Random Forest regressor\",\n",
        "    \"Bagging regressor\",\n",
        "    \"Tuned Decision Tree regressor\",\n",
        "    \"Tuned Bagging Tree regressor\",\n",
        "    \"Tuned Random Forest Regressor\"]\n",
        "\n",
        "print(\"Test performance comparison:\")\n",
        "models_test_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0DitToI2pMf"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- All the other models are giving a good performance in terms of RMSE and R-squared when compared to linear and ridge regression.\n",
        "- The bagging and random forest models are performing better than a single decision tree.\n",
        "- The random forest model with default parameters is giving the best performance among all the trained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR43ExubwV-n"
      },
      "source": [
        "### **Fitting the chosen final model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHyMjnZwWpx"
      },
      "source": [
        "- Final Model Building - Random Forest Regressor\n",
        "- We will consider Random Forest Regressor with default parameters as our final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmQYd6r_vxsz"
      },
      "outputs": [],
      "source": [
        "final_model = RandomForestRegressor(n_estimators = 100, random_state = 1)\n",
        "final_model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDAl8XtNvxu4"
      },
      "outputs": [],
      "source": [
        "final_model_perf_train = model_performance_regression(final_model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr9nsi1kzXh3"
      },
      "outputs": [],
      "source": [
        "final_model_perf_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLv7Ihr9wz73"
      },
      "source": [
        "* **Both the R-squared and the Adjusted R-squared of the model are approx 99.6% on the training data.** This indicates that the model is able to explain approx full variance in the target variable using the independent variables.\n",
        "\n",
        "* Let's do a quick performance check on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raTSMaGtvxwl"
      },
      "outputs": [],
      "source": [
        "final_model_test_perf = model_performance_regression(final_model, x_test, y_test)\n",
        "print(\"Test Performance:\")\n",
        "final_model_test_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk02N6Q_w92q"
      },
      "source": [
        "* The model is giving good performance on the test data as well i.e. the model is giving generalized performance.\n",
        "* The units of both RMSE and MAE are the same, days in this case. But RMSE is greater than MAE because it penalizes the outliers more.\n",
        "* **The MAE < 1 indicates that the model is able to predict the length of stay within a mean error of 1 day,** which is a very good performance.\n",
        "* **MAPE of 7.31 on the test data indicates that the model can predict within ~7% of the actual length of stay of patients.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpctb2GgwLAY"
      },
      "source": [
        "## **Observations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRC6OfhlwByu"
      },
      "source": [
        "\n",
        "\n",
        "1.   Random Forest Regressor model gives the best results. It is successful in capturing ~98% of variations in the test data.\n",
        "2.   MAE of 0.75 indicates that the model can succesfully predict the length of stay of a patient during admission with just an error of 1 day. \n",
        "3. Factors like visitors with **patient** and **admission deposit** plays an important role in the prediction.\n",
        "4. Factors like **staff_available** and **extra rooms** has very less to do with the predition from the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYlv2xRvvzFr"
      },
      "source": [
        "## **Business Insights and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPhjEiZ8v1Ub"
      },
      "source": [
        "- Gynecology is the busiest department of the hospital and it handles 68.7% of the total number of patients. It needs ample resources and staff for the smooth functioning of the department.\n",
        "- The number of visitors with the patients highly influences the length of stay of a patient. The maximum number of visitors can go up to 32 which is very high. A restriction can be imposed on this.\n",
        "- 74.2% of the patients are female. Thus, resources need to be procured while keeping this figure in mind.\n",
        "- A large number of patients (89.3%) of the patients are in trauma or emergency during admission. An increase in ambulances and emergency rooms can reduce the risk of casualties.\n",
        "- Ward A has the most number of patients who stay for the longest and the most serious patients. These wards can be equipped with more resources and staff to reduce the length of stay of these patients.\n",
        "- Elderly patients (51-100) and children (1-10) stay for the longest. Extra attention to these age groups can lead to a faster discharge from the hospital.\n",
        "- Wards D, E, and C have the most visitors with a patient. These wards will need more space and amenities like washrooms, shops, and lobbies for the visitors. Spaces can also be rented out to shop owners and advertisements to generate extra income.\n",
        "- Finally, the Random Forest Regressor can predict the length of stay of the patient with just an error of 1 day. The hospital can use these predictions to allocate the resources and staff accordingly and reduce any kind of wastage. The hospital can also allocate the wards and doctors accordingly to optimize admissions even during emergencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NCsvoPRS5rZ"
      },
      "source": [
        "## **Additional Content (Optional)**\n",
        "\n",
        "## **Boosting Models**\n",
        "\n",
        "Let's now look at the other kind of Ensemble technique - Boosting Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbgGiDdslXUn"
      },
      "source": [
        "### **XGBoost**\n",
        "- XGBoost stands for Extreme Gradient Boosting.\n",
        "- XGBoost is a tree-based ensemble machine learning technique that improves prediction power and performance by improvising on the Gradient Boosting framework and incorporating some reliable approximation algorithms. It is widely utilized and routinely appears at the top of competition leader boards in data science.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEI6-ZDoYkw8"
      },
      "outputs": [],
      "source": [
        "#installing the xgboost library using \"pip' command.\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hky6XMUzYi0S"
      },
      "outputs": [],
      "source": [
        "#importing the Random Forest Regressor and Bagging Regressor [Bagging]\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
        "\n",
        "#importing the AdaBoostRegressor and GradientBoostingRegressor [Boosting]\n",
        "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
        "\n",
        "#importing the XGBReressor from the xgboost\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbXxkU2dmNym"
      },
      "outputs": [],
      "source": [
        "#Adaboost Regressor\n",
        "adaboost_model = AdaBoostRegressor(random_state=1)\n",
        "\n",
        "#Fitting the model\n",
        "adaboost_model.fit(x_train,y_train)\n",
        "\n",
        "# model Performance on test data i.e prediction\n",
        "adaboost_model_perf_test = model_performance_regression(adaboost_model, x_test, y_test)\n",
        "\n",
        "adaboost_model_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quTWFI5eornN"
      },
      "outputs": [],
      "source": [
        "#Gradient Boost Regressor\n",
        "gbc = GradientBoostingRegressor(random_state=1)\n",
        "gbc.fit(x_train,y_train)\n",
        "gbc_perf_test = model_performance_regression(gbc, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4ioVaEhVL6T"
      },
      "outputs": [],
      "source": [
        "gbc_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPyN6_CVoxYy"
      },
      "outputs": [],
      "source": [
        "#XGBoost Regressor\n",
        "xgb = XGBRegressor(random_state=1, eval_metric='logloss')\n",
        "xgb.fit(x_train,y_train)\n",
        "xgb_perf_test = model_performance_regression(xgb, x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkmdwbaKVRim"
      },
      "outputs": [],
      "source": [
        "xgb_perf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wal3dM_29yLW"
      },
      "source": [
        "### **Hyperparameter Tuning: Boosting**\n",
        "\n",
        "Hyperparameter tuning is a great technique in machine learning to develop the model with optimal parameters. If the size of the data increases, the computation time will increase during training process.\n",
        "- For the practice purposes, we have listed below some of the important hyperparameters for each algorithm that can be tuned to improve the model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btmWTNY26aRq"
      },
      "source": [
        "**1. Adaboost**\n",
        "\n",
        "- Some important hyperparameters that can be tuned:\n",
        "  - **base_estimator** object, default=None\n",
        "The base estimator from which the boosted ensemble is built. If None, then the base estimator is DecisionTreeRegressor initialized with max_depth=3.\n",
        "  - **n_estimators** int, default=50\n",
        "  The maximum number of estimators at which boosting is terminated. In the case of a perfect fit, the learning procedure is stopped early.\n",
        "  - **loss :** {â€˜linearâ€™, â€˜squareâ€™, â€˜exponentialâ€™}, default=â€™linearâ€™\n",
        "The loss function to use when updating the weights after each boosting iteration.\n",
        " - **learning_rate** float, default=1.0\n",
        "Weight applied to each regressor at each boosting iteration. A higher learning rate increases the contribution of each regressor.\n",
        "\n",
        "**2. Gradient Boosting Algorithm**\n",
        "\n",
        "- - Some important hyperparameters that can be tuned:\n",
        "   - **n_estimators**: the number of boosting stages that will be performed.\n",
        " - **max_depth**: limits the number of nodes in the tree. The best value depends on the interaction of the input variables.\n",
        "\n",
        "  - **min_samples_split**: the minimum number of samples required to split an internal node.\n",
        "\n",
        "  - **learning_rate**: how much the contribution of each tree will shrink.\n",
        "\n",
        "  - **loss**: loss function to optimize. \n",
        "\n",
        "For a better understanding of each parameter in Gradient Boosting Regressor, please refer to this [source](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor).\n",
        "\n",
        "\n",
        "\n",
        "**3. XGBoost Algorithm**\n",
        "\n",
        "- - Some important hyperparameters that can be tuned:\n",
        "  - **booster** [default= gbtree ] Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree-based models while gblinear uses linear functions.\n",
        "  - **min_child_weight** [default=1]\n",
        "\n",
        "    The minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In the linear regression task, this simply corresponds to the minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be.\n",
        "\n",
        "  For a better understanding of each parameter in XGBoost Regressor, please refer to this [source](https://xgboost.readthedocs.io/en/stable/parameter.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icuGOdgBg9ZL"
      },
      "source": [
        "### **Comparison of all the models we have built so far**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU4hsMH4d_tk"
      },
      "outputs": [],
      "source": [
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        lin_reg_test.T,\n",
        "        ridge_regression_perf_test.T,\n",
        "        dt_regressor_perf_test.T,\n",
        "        regressor_perf_test.T,\n",
        "        bagging_estimator_perf_test.T,\n",
        "        adaboost_model_perf_test.T,\n",
        "        gbc_perf_test.T,\n",
        "        xgb_perf_test.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "models_test_comp_df.columns = [\n",
        "    \"Linear Regression\",\n",
        "    \"Ridge Regression\",\n",
        "    \"Decision tree regressor\",\n",
        "    \"Random Forest regressor\",\n",
        "    \"Bagging regressor\",\n",
        "    \"Adaboost regressor\",\n",
        "    \"Gradientboost regressor\",\n",
        "    \"XGBoost regressor\"\n",
        "]\n",
        "\n",
        "print(\"Test performance comparison:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEHFbSbZVYHF"
      },
      "outputs": [],
      "source": [
        "models_test_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxMEPVeETRpV"
      },
      "source": [
        "**Observations :**\n",
        "\n",
        "* With default parameters, the bagging methods outperform the boosting methods.\n",
        "* **The Random Forest Regressor gives the best performance for this dataset.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vh6pbSWLJ7pT",
        "uUEuNqtbPVT0",
        "wTHU4IPogofK",
        "gU_-9VCyskuV",
        "jeEmAqhGBlQ3",
        "zwYRba3GCmMR",
        "dnawqWwiDGAN",
        "n5PnzTiBuJA5",
        "fTjAF7lA2A9v",
        "wwDclxhA8ply",
        "GtfJkhgr2pMP",
        "hvDa_4-i2pMP",
        "16fOWNF62pMS",
        "RVWP-AHZ2pMT",
        "QDggSWcb2pMV",
        "ZOXNtE552pMW",
        "J0Nkflof9xLs",
        "sa8z6Ric93Qc",
        "lkXRAKBseIfT",
        "3h_IMznmkhTg",
        "eZg2DYmNeQ-2",
        "3u0gWZX1obu7",
        "XR43ExubwV-n",
        "Dpctb2GgwLAY",
        "mYlv2xRvvzFr",
        "cbgGiDdslXUn",
        "Wal3dM_29yLW",
        "icuGOdgBg9ZL"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}