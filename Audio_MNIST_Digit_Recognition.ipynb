{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PSamita/MIT-Course/blob/main/Audio_MNIST_Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe423ce5",
      "metadata": {
        "id": "fe423ce5"
      },
      "source": [
        "# **Audio MNIST Digit Recognition**\n",
        "\n",
        "---------------------\n",
        "## **Context**\n",
        "---------------------\n",
        "\n",
        "In the past decades, significant advances have been achieved in the area of audio recognition and a lot of research is going on globally to recognize audio data or speech using Deep Learning. The most common use case in this field is converting audio to spectrograms and vice versa. \n",
        "\n",
        "Audio in its raw form is usually a wave and to capture that using a data structure, we need to have a huge array of amplitudes even for a very short audio clip. Although it depends on the sampling rate of the sound wave, this structured data conversion for any audio wave is very voluminous even for low sampling rates. So it becomes a problem to store and computationally very expensive to do even simple calculations on such data.\n",
        "\n",
        "One of the best economical alternatives to this is using spectrograms. Spectrograms are created by doing Fourier or Short Time Fourier Transforms on sound waves. There are various kinds of spectrograms but the ones we will be using are called MFCC spectrograms. To put it in simple terms, a spectrogram is a way to visually encapsulate audio data. It is a graph on a 2-D plane where the X-axis represents time and the Y-axis represents Mel Coefficients. But since it is continuous on a 2-D plane, we can treat this as an image.\n",
        "\n",
        "\n",
        "---------------------\n",
        "## **Objective**\n",
        "---------------------\n",
        "\n",
        "The objective here is to build an Artificial Neural Network that can look at Mel or MFCC spectrograms of audio files and classify them into 10 classes. The audio files are recordings of different speakers uttering a particular digit and the corresponding class to be predicted is the digit itself.\n",
        "\n",
        "---------------------\n",
        "## **Dataset**\n",
        "---------------------\n",
        "\n",
        "The dataset we will use is the **Audio MNIST dataset**, which has audio files (having .wav extension) stored in 10 different folders. Each folder consists of these digits spoken by a particular speaker."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b776956b",
      "metadata": {
        "id": "b776956b"
      },
      "source": [
        "## **Understanding the required packages**\n",
        "\n",
        "* `Librosa`: Librosa is a Python package that helps in dealing with audio data. **librosa.display** visualizes and displays the audio data using Matplotlib. Similarly, there exists a collection of submodules under librosa that provides various other functionalities. **Run the command in the below cell to install the library**. \n",
        "* `IPython.display`: Display is a public API to display the tools available in Ipython. In this case study, we will create an audio object to display the digits in the MNIST audio data. \n",
        "* `tqdm`: tqdm is a Python package that allows us to add a progress bar to our application. This package will help us in iterating over the audio data. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f66b221",
      "metadata": {
        "id": "1f66b221"
      },
      "source": [
        "### **Installing Librosa**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2e0229",
      "metadata": {
        "id": "7d2e0229"
      },
      "outputs": [],
      "source": [
        "!pip install librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4aa6e90",
      "metadata": {
        "id": "e4aa6e90"
      },
      "source": [
        "## **Importing the necessary libraries and loading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a43151",
      "metadata": {
        "id": "09a43151"
      },
      "outputs": [],
      "source": [
        "# For Audio Preprocessing\n",
        "import librosa\n",
        "import librosa.display as dsp\n",
        "from IPython.display import Audio\n",
        "\n",
        "# For Data Preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# For Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#The data is provided as a zip file\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f72754f9",
      "metadata": {
        "id": "f72754f9"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"dark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Y6GYaH27lUj",
      "metadata": {
        "id": "4Y6GYaH27lUj"
      },
      "source": [
        "### **Mounting the Drive and Unzipping the file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L177QpGrN6QO",
      "metadata": {
        "id": "L177QpGrN6QO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dq8MJINQ2YFZ",
      "metadata": {
        "id": "Dq8MJINQ2YFZ"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Audio_MNIST_Archive.zip'\n",
        "\n",
        "#The data is provided as a zip file so we need to extract the files from the zip file\n",
        "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d399f22",
      "metadata": {
        "id": "8d399f22"
      },
      "source": [
        "### **Let's read and check some of the audio samples**\n",
        "\n",
        "The below function called \"get_audio\" takes a digit as an argument and plots the audio wave and returns the audio for a given digit. \n",
        "\n",
        "Let's understand the functioning of some of the new functions used to create the get_audio() function.\n",
        "\n",
        "* `.wav`: .wav is a file format like .csv which stores the raw audio format. We will load the .wav file using the librosa package. \n",
        "* `dsp.waveshow()`: It visualizes the waveform in the time domain. This method creates a plot that alternates between a raw samples-based view of the signal and an amplitude-envelope view of the signal. The \"sr\" parameter is the sampling rate, i.e., samples per second. \n",
        "* `Audio()`: From the Ipython package, we can create an audio object. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8da2ba",
      "metadata": {
        "id": "aa8da2ba"
      },
      "outputs": [],
      "source": [
        "def get_audio(digit = 0):\n",
        "\n",
        "    # Audio Sample Directory\n",
        "    sample = np.random.randint(1, 10)\n",
        "\n",
        "    # Index of Audio\n",
        "    index = np.random.randint(1, 5)\n",
        "    \n",
        "    # Modified file location\n",
        "    if sample < 10:\n",
        "        file = f\"/content/data/0{sample}/{digit}_0{sample}_{index}.wav\"\n",
        "\n",
        "    else:\n",
        "        file = f\"/content/data/{sample}/{digit}_{sample}_{index}.wav\"\n",
        "\n",
        "    \n",
        "    # Get Audio from the location\n",
        "    # Audio will be automatically resampled to the given rate (default sr = 22050)\n",
        "    data, sample_rate = librosa.load(file)\n",
        "    \n",
        "    # Plot the audio wave\n",
        "    dsp.waveshow(data, sr = sample_rate)\n",
        "    plt.show()\n",
        "    \n",
        "    # Show the widget\n",
        "    return Audio(data = data, rate = sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ohLAYFtP0IA",
      "metadata": {
        "id": "_ohLAYFtP0IA"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 0\n",
        "get_audio(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2357791b",
      "metadata": {
        "id": "2357791b"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 1\n",
        "get_audio(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4566f0b4",
      "metadata": {
        "id": "4566f0b4"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 2\n",
        "get_audio(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e368adb",
      "metadata": {
        "id": "1e368adb"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 3\n",
        "get_audio(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3ebff6",
      "metadata": {
        "id": "0e3ebff6"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 4\n",
        "get_audio(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa89676",
      "metadata": {
        "id": "dfa89676"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 5\n",
        "get_audio(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe199fe",
      "metadata": {
        "id": "efe199fe"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 6\n",
        "get_audio(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce9bea5",
      "metadata": {
        "id": "3ce9bea5"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 7\n",
        "get_audio(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b8e9c7",
      "metadata": {
        "id": "d9b8e9c7"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 8\n",
        "get_audio(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362acdae",
      "metadata": {
        "id": "362acdae"
      },
      "outputs": [],
      "source": [
        "# Show the audio and plot of digit 9\n",
        "get_audio(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "932a38b8",
      "metadata": {
        "id": "932a38b8"
      },
      "source": [
        "**Observations:** \n",
        "\n",
        "- The X-axis represents time and Y-axis represents the amplitude of the vibrations. The intuition behind the Fourier Transform is that any wave can be broken down or deconstructed as a sum of many composite sine waves. Since these are composed of sine waves, they are symmetric about the time axis, i.e, they extend equally above and below the time axis at a particular time. \n",
        "- From the various audio plots ranging from 0 to 9, we can observe the amplitude at a given point in time. For example, when we say \"Zero\", the \"Z\" sound has low amplitude and the \"ero\" sound has higher amplitude. Similarly, the remaining digits can be interpreted by looking at the visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42000d3c",
      "metadata": {
        "id": "42000d3c"
      },
      "source": [
        "## **Visualizing the spectrogram of the audio data**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5b20eb",
      "metadata": {
        "id": "cb5b20eb"
      },
      "source": [
        "### **What is a spectrogram?**\n",
        "\n",
        "A spectrogram is a visual way of representing the signal strength or “loudness” of a signal over time at various frequencies or time steps present in a particular waveform. A spectrogram gives a detailed view of audio. It represents amplitude, frequency, and time in a single plot. Since spectrograms are continuous plots, they can be interpreted as an image. Different spectrograms have different attributes on their axes and they are usually different to interpret. In a Research and Development scenario, we make use of a vocoder, which is an encoder that converts spectrograms back to audio using parameters learned by machine learning. One great vocoder is the WaveNet vocoder which is used in almost all Text to Speech architectures.\n",
        "\n",
        "Here, we will be using **MFCC** spectrograms, which are also called **Mel** spectrograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2857114",
      "metadata": {
        "id": "b2857114"
      },
      "outputs": [],
      "source": [
        "# A function which returns audio file for a mentioned digit\n",
        "def get_audio_raw(digit = 0):\n",
        "\n",
        "    # Audio Sample Directory\n",
        "    sample = np.random.randint(1, 10)\n",
        "\n",
        "    # Index of Audio\n",
        "    index = np.random.randint(1, 5)\n",
        "    \n",
        "    # Modified file location\n",
        "    if sample < 10:\n",
        "        file = f\"/content/data/0{sample}/{digit}_0{sample}_{index}.wav\"\n",
        "\n",
        "    else:\n",
        "        file = f\"/content/data/{sample}/{digit}_{sample}_{index}.wav\"\n",
        "\n",
        "    \n",
        "    # Get Audio from the location\n",
        "    data, sample_rate = librosa.load(file)\n",
        "\n",
        "    # Return audio\n",
        "    return data, sample_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0be1f3",
      "metadata": {
        "id": "1b0be1f3"
      },
      "source": [
        "### **Extracting features from the audio file**\n",
        "\n",
        "**Mel-frequency cepstral coefficients (MFCCs) Feature Extraction**\n",
        "\n",
        "MFCCs are usually the final features used in many machine learning models trained on audio data. They are usually a set of mel coefficients defined for each time step through which the raw audio data can be encoded. So for example, if we have an audio sample extending for 30 time steps, and we are defining each time step by 40 Mel Coefficients, our entire sample can be represented by 40 * 30 Mel Coefficients. And if we want to create a Mel Spectrogram out of it, our spectrogram will resemble a 2-D array of 40 horizontal rows and 30 vertical columns.\n",
        "\n",
        "In this time step, we will first extract the Mel Coefficents for each audio file and add them to our dataset.\n",
        "\n",
        "* `extract_features` : Returns the MFCC extracted features for an audio file. \n",
        "* `process_and_create_dataset` : Iterate through the audio of each digit, extract the features using the extract_features() function, and append the data into a DataFrame. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f597acf",
      "metadata": {
        "id": "3f597acf"
      },
      "source": [
        "**Creating a function that extracts the data from audio files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8deccefb",
      "metadata": {
        "id": "8deccefb"
      },
      "outputs": [],
      "source": [
        "# Will take an audio file as input and return extracted features using MEL_FREQUENCY CEPSTRAL COEFFICIENT as the output\n",
        "def extract_features(file):\n",
        "\n",
        "    # Load audio and its sample rate\n",
        "    audio, sample_rate = librosa.load(file)\n",
        "\n",
        "    # Extract features using mel-frequency coefficient\n",
        "    extracted_features = librosa.feature.mfcc(y = audio,\n",
        "                                              sr = sample_rate,\n",
        "                                              n_mfcc = 40)\n",
        "    \n",
        "    # Scale the extracted features\n",
        "    extracted_features = np.mean(extracted_features.T, axis = 0)\n",
        "\n",
        "    # Return the extracted features\n",
        "    return extracted_features\n",
        "\n",
        "\n",
        "def preprocess_and_create_dataset():\n",
        "\n",
        "    # Path of the folder where the audio files are present\n",
        "    root_folder_path = \"/content/data/\"\n",
        "\n",
        "    # Empty List to create dataset\n",
        "    dataset = []\n",
        "    \n",
        "    # Iterating through folders where each folder has the audio of each digit\n",
        "    for folder in tqdm(range(1, 11)):\n",
        "\n",
        "        if folder < 10:\n",
        "\n",
        "            # Path of the folder\n",
        "            folder = os.path.join(root_folder_path, \"0\" + str(folder))\n",
        "\n",
        "        else:\n",
        "            folder = os.path.join(root_folder_path, str(folder))\n",
        "            \n",
        "        # Iterate through each file of the present folder\n",
        "        for file in tqdm(os.listdir(folder)):\n",
        "\n",
        "            # Path of the file\n",
        "            abs_file_path = os.path.join(folder, file)\n",
        "\n",
        "            # Pass path of file to the extracted_features() function to create features\n",
        "            extracted_features = extract_features(abs_file_path) \n",
        "\n",
        "            # Class of the audio, i.e., the digit it represents\n",
        "            class_label = file[0]\n",
        "            \n",
        "            # Append a list where the feature represents a column and class of the digit represents another column\n",
        "            dataset.append([extracted_features, class_label])\n",
        "    \n",
        "    # After iterating through all the folders, convert the list to a DataFrame\n",
        "    return pd.DataFrame(dataset, columns = ['features', 'class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88643b8",
      "metadata": {
        "id": "a88643b8"
      },
      "source": [
        "**Now. let's create the dataset using the defined function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65def686",
      "metadata": {
        "id": "65def686"
      },
      "outputs": [],
      "source": [
        "# Create the dataset by calling the function\n",
        "dataset = preprocess_and_create_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5316bccd",
      "metadata": {
        "id": "5316bccd"
      },
      "source": [
        "**View first 5 rows of the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88444596",
      "metadata": {
        "id": "88444596"
      },
      "outputs": [],
      "source": [
        "# View the head of the DataFrame\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e4ec8ad",
      "metadata": {
        "id": "8e4ec8ad"
      },
      "outputs": [],
      "source": [
        "# Storing the class as int \n",
        "dataset['class'] = [int(x) for x in dataset['class']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6aff2c5",
      "metadata": {
        "id": "b6aff2c5"
      },
      "outputs": [],
      "source": [
        "# Check the frequency of classes in the dataset\n",
        "dataset['class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c4b3dd",
      "metadata": {
        "id": "f1c4b3dd"
      },
      "source": [
        "### **Visualizing the Mel Frequency Cepstral Coefficients Using a Spectrogram**\n",
        "\n",
        "* `draw_spectrograms` : From the Mel Coefficients we are extracting for a particular audio, this function is creating the 2-D graph of those coefficients with the X-axis representing time and the Y-axis shows the corresponding Mel coefficients in that time step. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba4ce83",
      "metadata": {
        "id": "dba4ce83"
      },
      "outputs": [],
      "source": [
        "# A function which returns MFCC\n",
        "def draw_spectrograms(audio_data, sample_rate):\n",
        "\n",
        "    # Extract features\n",
        "    extracted_features = librosa.feature.mfcc(y = audio_data,\n",
        "                                              sr = sample_rate,\n",
        "                                              n_mfcc = 40)\n",
        "    \n",
        "    # Return features without scaling\n",
        "    return extracted_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c30b79ee",
      "metadata": {
        "id": "c30b79ee"
      },
      "source": [
        "The very first MFCC coefficient (0th coefficient) does not provide information about the overall shape of the spectrum. It simply communicates a constant offset or the addition of a constant value to the full spectrum. As a result, when performing classification, many practitioners will disregard the initial MFCC. In the images, you can see those represented by blue pixels.\n",
        "\n",
        "We can plot the MFCCs, but it's difficult to tell what kind of signal is hiding behind such representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f51432",
      "metadata": {
        "id": "24f51432",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Creating subplots\n",
        "fig, ax = plt.subplots(5, 2, figsize = (15, 30))\n",
        "\n",
        "# Initializing row and column variables for subplots\n",
        "row = 0\n",
        "column = 0\n",
        "\n",
        "for digit in range(10):\n",
        "\n",
        "    # Get the audio of different classes (0-9)\n",
        "    audio_data, sample_rate = get_audio_raw(digit)\n",
        "    \n",
        "    # Extract their MFCC\n",
        "    mfcc = draw_spectrograms(audio_data, sample_rate)\n",
        "    print(f\"Shape of MFCC of audio digit {digit} ---> \", mfcc.shape)\n",
        "    \n",
        "    # Display the plots and its title\n",
        "    ax[row,column].set_title(f\"MFCC of audio class {digit} across time\")\n",
        "    librosa.display.specshow(mfcc, sr = 22050, ax = ax[row, column])\n",
        "    \n",
        "    # Set X-labels and Y-labels\n",
        "    ax[row,column].set_xlabel(\"Time\")\n",
        "    ax[row,column].set_ylabel(\"MFCC Coefficients\")\n",
        "    \n",
        "    # Conditions for positioning of the plots\n",
        "    if column == 1:\n",
        "        column = 0\n",
        "        row += 1\n",
        "    else:\n",
        "        column+=1\n",
        "        \n",
        "    \n",
        "plt.tight_layout(pad = 3)   \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UylNJ85WA5Qq",
      "metadata": {
        "id": "UylNJ85WA5Qq"
      },
      "source": [
        "**Visual Inspection of MFCC Spectrograms:**\n",
        "\n",
        "On inspecting them visually, we can see that there are a lot of deviations from the spectrograms of one audio to another. There are a lot of tiny rectangles and bars whose positions are unique to each audio. So, the Artificial Neural Network should be able to perform decently in identifying these audios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c589764c",
      "metadata": {
        "id": "c589764c"
      },
      "source": [
        "## **Perform Train-Test-Split**\n",
        "\n",
        "- Split the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14096ae4",
      "metadata": {
        "id": "14096ae4"
      },
      "outputs": [],
      "source": [
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.array(dataset['features'].to_list())\n",
        "Y = np.array(dataset['class'].to_list())\n",
        "\n",
        "# Create train set and test set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.75, shuffle = True, random_state = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca774642",
      "metadata": {
        "id": "ca774642"
      },
      "outputs": [],
      "source": [
        "# Checking the shape of the data\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b898414b",
      "metadata": {
        "id": "b898414b"
      },
      "source": [
        "## **Modelling**\n",
        "\n",
        "- Create an artificial neural network to recognize the digit. \n",
        "\n",
        "**About the libraries:** \n",
        "\n",
        "- `Keras`: Keras is an open-source deep-learning library in Python. Keras is popular because the API was clean and simple, allowing standard deep learning models to be defined, fit, and evaluated in just a few lines of code.\n",
        "- `Sklearn` :\n",
        "   * Simple and efficient tools for predictive data analysis\n",
        "   * Accessible to everybody, and reusable in various contexts\n",
        "   * Built on NumPy, SciPy, and matplotlib\n",
        "   * Open source, commercially usable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aee38cb",
      "metadata": {
        "id": "2aee38cb"
      },
      "source": [
        "### **Import necessary libraries for building the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4839d06",
      "metadata": {
        "id": "d4839d06"
      },
      "outputs": [],
      "source": [
        "# To create an ANN model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# To create a checkpoint and save the best model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# To load the model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# To evaluate the model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2b3a34",
      "metadata": {
        "id": "0b2b3a34"
      },
      "source": [
        "## **Model Creation**\n",
        "\n",
        "\n",
        "### **Why are we using ANN's?**\n",
        "\n",
        "\n",
        "When we are converting audios to their corresponding spectrograms, we will have similar spectrograms for similar audios irrespective of who the speaker is, and what is their pitch and timber like. So local spatiality is never going to be a problem. So having convolutional layers on top of our fully connected layers is just adding to our computational redundancy.\n",
        "\n",
        "We will use a Sequential model with multiple connected hidden layers, and an output layer that returns a single, continuous value.\n",
        "\n",
        "- A Sequential model is a linear stack of layers. Sequential models can be created by giving a list of layer instances. \n",
        "- A dense layer of neurons is a simple layer of neurons in which each neuron receives input from all of the neurons in the previous layer.\n",
        "- The most popular function employed for hidden layers is the rectified linear activation function, or ReLU activation function. It's popular because it's easy to use and effective in getting around the limitations of other popular activation functions like Sigmoid and Tanh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d68ffdb",
      "metadata": {
        "id": "0d68ffdb"
      },
      "outputs": [],
      "source": [
        "# Crete a Sequential Object\n",
        "model = Sequential()\n",
        "\n",
        "# Add first layer with 100 neurons to the sequental object\n",
        "model.add(Dense(100, input_shape = (40, ), activation = 'relu'))\n",
        "\n",
        "# Add second layer with 100 neurons to the sequental object\n",
        "model.add(Dense(100, activation = 'relu'))\n",
        "\n",
        "# Add third layer with 100 neurons to the sequental object\n",
        "model.add(Dense(100, activation = 'relu'))\n",
        "\n",
        "# Output layer with 10 neurons as it has 10 classes\n",
        "model.add(Dense(10, activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75043430",
      "metadata": {
        "id": "75043430"
      },
      "outputs": [],
      "source": [
        "# Print Summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1512eb",
      "metadata": {
        "id": "0a1512eb"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'],\n",
        "              optimizer = 'adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38bdc379",
      "metadata": {
        "id": "38bdc379"
      },
      "source": [
        "### **Model Checkpoint & Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6483ff5d",
      "metadata": {
        "id": "6483ff5d"
      },
      "outputs": [],
      "source": [
        "# Set the number of epochs for training\n",
        "num_epochs = 100\n",
        "\n",
        "# Set the batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = num_epochs, batch_size = batch_size, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf00a56",
      "metadata": {
        "id": "2bf00a56"
      },
      "source": [
        "### **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "450f4674",
      "metadata": {
        "id": "450f4674"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred = [np.argmax(i) for i in Y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c6be71",
      "metadata": {
        "id": "d3c6be71"
      },
      "outputs": [],
      "source": [
        "# Set style as dark\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize = (15, 8))\n",
        "\n",
        "# Plot the title\n",
        "plt.title(\"CONFUSION MATRIX FOR MNIST AUDIO PREDICTION\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix([int(x) for x in Y_test], Y_pred)\n",
        "\n",
        "# Plot the confusion matrix as heatmap\n",
        "sns.heatmap(cm, annot = True, cmap = \"cool\", fmt = 'g', cbar = False)\n",
        "\n",
        "# Set X-label and Y-label\n",
        "plt.xlabel(\"ACTUAL VALUES\")\n",
        "plt.ylabel(\"PREDICTED VALUES\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Print the metrics\n",
        "print(classification_report(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a0f6be",
      "metadata": {
        "id": "07a0f6be"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- From the confusion matrix, we can observe that most of the observations are correctly identified by the model. \n",
        "- In very few cases, the model is not able to identify the correct digit. For example, 9 observations are 0 but the model has predicted them as 2. \n",
        "- The model has given a great performance with 99% recall, precision and F1-score. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}